<!doctype html>
<html>
<head>
<meta charset='UTF-8'><meta name='viewport' content='width=device-width initial-scale=1'>
<title>集成学习</title>
</head>
<body><h1 id='集成学习'>集成学习</h1>
<h2 id='一三个臭皮匠抵过诸葛亮'>一、三个臭皮匠，抵过诸葛亮</h2>
<h3 id='1集成学习的原理'>1.集成学习的原理</h3>
<p>集成学习旨在通过结合多个分类器或回归器来提高预测准确性和鲁棒性。</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.08.21.png" alt="截屏2023-04-11 下午4.08.21" style="zoom:50%;" /></p>
<p>集成学习的基本思想是：将多个相对独立的学习器进行合理的结合，以达到更好的分类、回归效果。</p>
<p>集成学习可以通过多种方式实现，如投票、平均值、加权平均值、Bagging、Boosting等。其中，Bagging和Boosting是最常用的技术。</p>
<p>Bagging通过自助采样技术来构建多个相互独立的子集，每个子集训练一个基学习器，然后将它们的结果进行投票、平均值等方式进行集成。</p>
<p>Boosting则是通过逐步提高错误样本的权重来构建多个基学习器，并将它们的结果进行加权平均，以获得最终的预测结果。</p>
<p>集成学习可以显著提高预测准确性和鲁棒性，特别是在面对复杂数据集和高维数据时更加有效。</p>
<p>特点：</p>
<ol start='' >
<li>多个分类器集成在一起，以提高分类准确率。</li>
<li>有训练数据构建基分类器，然后根据预测结果投票。</li>
<li>集成学习本身不是分类器，而是将分类器结合起来的一种方法。</li>
<li>通常集成分类器性能会好于单个分类器。</li>
<li><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.10.30.png" alt="截屏2023-04-11 下午4.10.30" style="zoom:50%;" /></li>

</ol>
<h3 id='2bias-variance-tradeoff'>2.Bias-Variance tradeoff</h3>
<p>Bias-Variance tradeoff是指模型的预测误差可以分解为三个部分：偏差（Bias）、方差（Variance）和不可避免的误差。</p>
<p>偏差是指模型的预测值与真实值的差异，通常由于模型过于简单而无法捕捉数据中的复杂关系所导致。</p>
<p>方差则是指模型的预测值在不同数据集上的差异，通常由于模型过于复杂而对噪声数据过度拟合所导致。</p>
<p>不可避免的误差则是由于数据本身的噪声或不确定性所导致的误差。</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.12.24.png" alt="截屏2023-04-11 下午4.12.24" style="zoom:67%;" /></p>
<p>Bias-Variance tradeoff指的是在训练模型时，当我们尝试降低偏差时，往往会导致方差的增加，反之亦然。因此，我们需要在偏差和方差之间找到平衡点，以获得最佳的预测效果。当模型过于简单时，会出现高偏差和低方差的情况，此时模型的预测能力较差。当模型过于复杂时，会出现低偏差和高方差的情况，此时模型可能会对训练数据过度拟合，导致在新数据上的预测效果较差。</p>
<p>为了避免Bias-Variance tradeoff导致的过度拟合或欠拟合问题，我们可以采用一些技术来平衡模型的偏差和方差，如交叉验证、正则化、集成学习等。</p>
<h3 id='3结合策略'>3.结合策略</h3>
<p>集成学习的基学习器的结合方式主要有序列集成和并行集成两种：</p>
<ol start='' >
<li><p>序列集成</p>
<ol start='' >
<li>利用基学习器之间的依赖关系，依次生成基学习器的结果</li>
<li>可以减少偏差bias</li>

</ol>
</li>
<li><p>并行集成</p>
<ol start='' >
<li>利用基学习器之间的独立关系，并行生成基学习器的结果</li>
<li>可以减少方差variance</li>

</ol>
</li>

</ol>
<p>针对这两种结合方法，我们就需要考虑如何训练每个学习器和如何结合每个学习器，一般来说结合策略有：</p>
<ol start='' >
<li>平均法（回归问题）：简单平均，加权平均。</li>
<li>投票法（分类问题）：绝对多数，相对多数，加权投票。</li>
<li>学习法。</li>

</ol>
<h2 id='二bagging'>二、Bagging</h2>
<p>Bagging方法是通过随机构造训练样本、随机选择特征等方法来提高每个基模型的独立性。由于训练数据的不同，获得的学习器会存在差异性，但是若采样的每个子集都完全不同，则每个基学习器都只能训练一小部分数据，无法进行有效的学习。因此考虑使用相互交叠的采样子集。代表性方法有Bagging和随机森林等。</p>
<h3 id='baggingbootstrap-aggregating）'>Bagging（Bootstrap aggregating）</h3>
<p>Bagging（Bootstrap Aggregating）是通过不同模型的训练数据集的独立性来提高不同模型之间的独立性。</p>
<p>我们在原始训练集上进行有放回的随机采样，初始训练集中有的样本在采样集合中出现多次，有的则从未出现过。经过M次随机采样得到含m个样本的训练集。可以采样T个含有m个样本的数据集并行训练得到T个模型，然后将这些基学习模型进行结合。</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.33.48.png" alt="截屏2023-04-11 下午4.33.48" style="zoom:50%;" /></p>
<p>对于基学习器的集成方法，Bagging通常对分类任务使用简单投票法，对回归任务使用平均法。若预测的结果中有含有相同票数的两个类，可以使用随机选择或者考察学习器投票的置信度来确定。</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.34.00.png" alt="截屏2023-04-11 下午4.34.00" style="zoom:110%;" /></p>
<p>Bagging集成学习的优点：降低分类器方差，改善泛化。</p>
<ol start='' >
<li>Bagging的性能依赖于基分类器的稳定性，如果基分类器稳定，则其误差主要由基分类器的bias决定。</li>
<li>由于采样概率相同，bagging方法不侧重于任何特定实例。</li>

</ol>
<h3 id='随机森林'>随机森林</h3>
<p>随机森林（Random Forest）在Bagging的基础上再引入了随机特征，进一步提高每个基模型之间的独立性。</p>
<p>在随机森林中，每个基模型都是一棵决策树，与传统决策树不同的是，在RF中，对每个基决策树的每个节点，先从该节点的属性集合中随机选择一个包含k个属性的子集，然后从这个子集中选择一个最优属性用于划分，而传统的决策树是直接在当前节点的属性集合中选择一个最优属性来划分集合。</p>
<p>算法过程：</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.36.58.png" alt="截屏2023-04-11 下午4.36.58" style="zoom:100%;" /></p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.37.43.png" alt="截屏2023-04-11 下午4.37.43" style="zoom:100%;" /></p>
<h2 id='三boosting'>三、Boosting</h2>
<p>Boosting类方法是按照一定的顺序来先后训练不同的基模型，每个模型都针对先前模型的错误进行专门训练。</p>
<p>根据先前模型的结果，来调整训练样本的权重，从而增加不同基模型之间的差异性。</p>
<p>Boosting的过程很类似于人类学习的过程，我们学习新知识的过程往往是迭代式的。第一遍学习的时候，我们会记住一部分知识，但往往也会犯一些错误，对于这些错误，我们的印象会很深。第二遍学习的时候，就会针对犯过错误的知识加强学习，以减少类似的错误发生。不断循环往复，直到犯错误的次数减少到很低的程度。</p>
<p><img src="https://wbw2333.github.io/assets/imgs/%E6%88%AA%E5%B1%8F2023-04-11%20%E4%B8%8B%E5%8D%884.39.58.png" alt="截屏2023-04-11 下午4.39.58" style="zoom:100%;" /></p>
<p>&nbsp;</p>
</body>
</html>